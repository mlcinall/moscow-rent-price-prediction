{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {}
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Очистка, геокодирование и Feature Engineering\n",
    "\n",
    "В этом ноутбуке мы последовательно:\n",
    "\n",
    "1. **Загрузка и базовая очистка**  \n",
    "   - Считываем исходный файл  \n",
    "   - Удаляем нерелевантные признаки  \n",
    "   - Фильтруем записи с пустыми улицами и номерами дома  \n",
    "   - Инициализируем столбцы `lat`, `lon`, `geo_quality`\n",
    "\n",
    "2. **Геокодирование адресов (дом → улица)**  \n",
    "   1. Первый проход: точечный геокодинг на уровне дома (`geo_quality = 1`)  \n",
    "   2. Второй проход: центр улицы + кеширование (`geo_quality = 0`)\n",
    "\n",
    "3. **Заполнение районов и ближайшей станции метро**  \n",
    "   - Spatial-join точек (`lat`,`lon`) с полигонами районов → `district`  \n",
    "   - KD-Tree ближайших станций метро → `underground`\n",
    "\n",
    "4. **Очистка и сохранение результата**  \n",
    "   - Удаляем остаточные пропуски по координатам и районам  \n",
    "   - Сохраняем очищенный датасет в CSV\n",
    "\n",
    "5. **Расчёт дополнительных фичей**  \n",
    "   - Расстояние до **центра города**  \n",
    "   - Расстояние до **ближайшего метро** \n",
    "   - Наличие **парка** в радиусе 1 км  \n",
    "   - Количество объектов соц. инфраструктуры (школы, поликлиники, магазины, аптеки) в радиусе 1 км  \n",
    "\n",
    "---\n",
    "\n",
    "**Цель:** получить чистый, географически обогащённый и готовый к обучению ML-датасет для предсказания справедливой цены аренды.  "
   ],
   "metadata": {
    "id": "CwzOlEwrIUGi"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Импорт библиотек\n",
    "___"
   ],
   "metadata": {
    "id": "8yB7A7Hkv_V3"
   }
  },
  {
   "cell_type": "code",
   "source": "!pip install --quiet osmnx geopandas shapely numpy pandas scipy geopy tqdm joblib fiona osmium",
   "metadata": {
    "id": "FBuH9eLhvjHQ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "34d3e957-33a9-47dd-ebbb-07586d656872",
    "ExecuteTime": {
     "end_time": "2025-05-24T14:52:05.395491Z",
     "start_time": "2025-05-24T14:52:03.001451Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T14:52:14.399223Z",
     "start_time": "2025-05-24T14:52:05.398106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List, Tuple\n",
    "import json\n",
    "import os\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import osmium\n",
    "import pandas as pd\n",
    "import pyproj\n",
    "import requests\n",
    "import shapely.wkt\n",
    "import geopandas as gpd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.spatial import cKDTree\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "from geopy.exc import GeocoderTimedOut, GeocoderServiceError\n",
    "\n",
    "# Настройки\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "tqdm.pandas()"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Загрузка и базовая очистка данных\n",
    "\n",
    "*Что происходит:*  \n",
    "- Считываем исходный файл `cian_rent_Москва_balanced.xls`.  \n",
    "- Удаляем нерелевантные колонки (`USELESS_COLS`).  \n",
    "- Приводим пустые строки к `pd.NA` для ключевых столбцов (`LOC_COLS`).  \n",
    "- Фильтруем записи без улицы или номера дома.  \n",
    "- Инициализируем столбцы `lat`, `lon`, `geo_quality`.\n",
    "___\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Загрузка и базовая очистка датасета\n",
    "RAW_PATH = \"cian_rent_Moscow_balanced.xls\"\n",
    "\n",
    "USELESS_COLS = [\n",
    "    \"author\", \"author_type\", \"url\", \"location\",\n",
    "    \"deal_type\", \"accommodation_type\", \"commissions\",\n",
    "    \"residential_complex\",\n",
    "]\n",
    "LOC_COLS = [\"underground\", \"house_number\", \"street\", \"district\"]\n",
    "\n",
    "data_raw: pd.DataFrame = pd.read_csv(RAW_PATH)\n",
    "\n",
    "data_useful: pd.DataFrame = (\n",
    "    data_raw\n",
    "    .drop(USELESS_COLS, axis=1, errors=\"ignore\")\n",
    "    .assign(**{c: lambda df, c=c: df[c].replace(\"\", pd.NA) for c in LOC_COLS})\n",
    "    .dropna(subset=[\"street\", \"house_number\"])\n",
    "    .assign(lat=pd.NA, lon=pd.NA, geo_quality=pd.NA)          # geo_quality: 1 точно, 0 улица\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(f\"После очистки: {len(data_useful):,} объявлений\")"
   ],
   "metadata": {
    "id": "ngfefiAuIFC5",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d6404342-5cd5-4306-b068-f868143644f2",
    "ExecuteTime": {
     "end_time": "2025-05-24T14:52:17.049724Z",
     "start_time": "2025-05-24T14:52:16.881611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "После очистки: 16,114 объявлений\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Загрузка геоданных: районы и метро\n",
    "\n",
    "*Что происходит:*  \n",
    "- Загружаем полигоны районов Москвы из GeoJSON → `districts`.  \n",
    "- Переводим CRS в `EPSG:4326` и переименовываем колонку района в `district_name`.  \n",
    "- Скачиваем или читаем CSV со станциями метро, строим `cKDTree` для быстрого поиска ближайшей станции.\n",
    "\n",
    "___\n"
   ],
   "metadata": {
    "id": "-hLubJKdMqrJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def load_districts_and_metro(\n",
    "    districts_path: str = \"mo.geojson\",\n",
    "    metro_csv_path: str = \"moscow_metro_stations.csv\"\n",
    ") -> tuple[gpd.GeoDataFrame, pd.DataFrame, cKDTree]:\n",
    "    # Загрузка и обработка районов\n",
    "    def rename_columns(c: str) -> str:\n",
    "        cl = c.lower()\n",
    "        if cl.startswith(\"name\") and c != \"geometry\":\n",
    "            return \"district_name\"\n",
    "        elif cl.startswith(\"adm\") or \"округ\" in cl or \"area\" in cl:\n",
    "            return \"adm_area\"\n",
    "        return c\n",
    "\n",
    "    districts = (\n",
    "        gpd.read_file(districts_path)\n",
    "        .to_crs(\"EPSG:4326\")\n",
    "        .rename(columns=rename_columns)\n",
    "    )\n",
    "\n",
    "    # Только нужные колонки\n",
    "    district_cols = [c for c in [\"district_name\", \"adm_area\", \"geometry\"] if c in districts.columns]\n",
    "    districts = districts[district_cols]\n",
    "\n",
    "    # Загрузка и обработка метро\n",
    "    metro_csv = Path(metro_csv_path)\n",
    "    if not metro_csv.exists():\n",
    "        print(\"Скачиваем станции метро из API HeadHunter...\")\n",
    "        response = requests.get(\"https://api.hh.ru/metro/1\")\n",
    "        data = response.json()\n",
    "        stations = []\n",
    "        for line in data['lines']:\n",
    "            for station in line['stations']:\n",
    "                stations.append({\n",
    "                    'name': station['name'],\n",
    "                    'lat': station['lat'],\n",
    "                    'lon': station['lng']\n",
    "                })\n",
    "        metro_df = pd.DataFrame(stations)\n",
    "        metro_df.to_csv(metro_csv, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"Станции метро сохранены в {metro_csv_path}\")\n",
    "    else:\n",
    "        metro_df = pd.read_csv(metro_csv)\n",
    "\n",
    "    # Убираем все лишнее\n",
    "    metro_df = metro_df[[\"name\", \"lat\", \"lon\"]]\n",
    "\n",
    "    # KD-дерево\n",
    "    metro_tree = cKDTree(metro_df[[\"lat\", \"lon\"]].to_numpy())\n",
    "\n",
    "    return districts, metro_df, metro_tree"
   ],
   "metadata": {
    "id": "P3ebnQlyZ1p5",
    "ExecuteTime": {
     "end_time": "2025-05-24T14:52:30.816047Z",
     "start_time": "2025-05-24T14:52:30.803960Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "districts, metro_df, metro_tree = load_districts_and_metro()"
   ],
   "metadata": {
    "id": "n9T4reQqYmce",
    "ExecuteTime": {
     "end_time": "2025-05-24T14:52:37.389985Z",
     "start_time": "2025-05-24T14:52:35.394313Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Нормализация адресов\n",
    "\n",
    "*Что происходит:*  \n",
    "- Убираем префиксы улиц (`ул.`, `проспект`, `г.`, и т.п.) при помощи регулярного выражения.  \n",
    "- Удаляем точки и запятые, приводим к нижнему регистру.  \n",
    "- Формируем строку вида `Россия, Москва, <улица> <дом>`, готовую для геокодера.\n",
    "\n",
    "___\n"
   ],
   "metadata": {
    "id": "2WD2Y-CVPLeE"
   }
  },
  {
   "metadata": {
    "id": "cFQZ-Xm7N0de",
    "ExecuteTime": {
     "end_time": "2025-05-24T14:53:08.488698Z",
     "start_time": "2025-05-24T14:53:08.475709Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Нормализация адреса\n",
    "_addr_head = re.compile(r\"^\\s*(ул\\.?|улица|проспект|пр[-‐-]?(кт|т)|г\\.?|город)\\s*\", flags=re.I)\n",
    "\n",
    "def normalize_address(row: pd.Series) -> str:\n",
    "    street = _addr_head.sub(\"\", str(row.street).strip().lower())\n",
    "    street = re.sub(r\"[.,]\", \" \", street).strip()\n",
    "    return f\"Россия, Москва, {street} {row.house_number}\""
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Геокодирование адресов (двухэтапное)\n",
    "\n",
    "*Что происходит:*  \n",
    "1. **Первый проход (house-level):**  \n",
    "   - Геокодим каждый адрес на уровне дома через Nominatim.  \n",
    "   - Помечаем `geo_quality = 1`.  \n",
    "2. **Второй проход (street-level):**  \n",
    "   - Собираем уникальные названия улиц без координат, геокодим центры улиц, кешируем.  \n",
    "   - Подставляем из кеша в оставшиеся пропуски, `geo_quality = 0`.  \n",
    "3. Удаляем все записи, у которых так и не появились координаты.\n",
    "\n",
    "___"
   ],
   "metadata": {
    "id": "XIVt_2e2PPRt"
   }
  },
  {
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "f6a14bb6c1e1479783587df8cbe79623",
      "449c91ad267d4ccf971e4f7642d4ac61",
      "147b87f2918246c1a9d2840899ca973c"
     ]
    },
    "id": "JAssL-86N0df",
    "outputId": "b7aea9f6-16b4-4c57-aed5-7b50ee0d6766",
    "ExecuteTime": {
     "end_time": "2025-05-24T20:54:51.583055Z",
     "start_time": "2025-05-24T14:53:15.244878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Геокодер и двухэтапное заполнение координат\n",
    "geocoder = Nominatim(user_agent=\"rental-geocoder\", timeout=10)\n",
    "rate_limited = RateLimiter(geocoder.geocode, min_delay_seconds=1.1, swallow_exceptions=False)\n",
    "\n",
    "GOOD_TYPES: set[str] = {\"house\", \"building\", \"apartments\", \"residential\", \"yes\"}\n",
    "STREET_TYPES: set[str] = {\"residential\", \"road\", \"street\", \"tertiary\",\n",
    "                          \"secondary\", \"primary\", \"motorway\", \"service\"}\n",
    "\n",
    "def geocode_rows(rows: Iterable[Tuple[int, pd.Series]],\n",
    "                 street_cache: Dict[str, Tuple[float, float]] | None = None) -> None:\n",
    "    \"\"\"Заполняет lat/lon и geo_quality в исходном data_useful по индексам rows.\"\"\"\n",
    "    for idx, row in rows:\n",
    "        try:\n",
    "            loc = rate_limited(normalize_address(row))\n",
    "        except (GeocoderTimedOut, GeocoderServiceError):\n",
    "            continue\n",
    "\n",
    "        if loc and loc.raw.get(\"type\") in GOOD_TYPES:\n",
    "            data_useful.at[idx, \"lat\"] = loc.latitude\n",
    "            data_useful.at[idx, \"lon\"] = loc.longitude\n",
    "            data_useful.at[idx, \"geo_quality\"] = 1\n",
    "        elif street_cache is not None:\n",
    "            s_key = str(row.street).lower().strip()\n",
    "            coords = street_cache.get(s_key)\n",
    "            if coords:\n",
    "                data_useful.at[idx, \"lat\"], data_useful.at[idx, \"lon\"] = coords\n",
    "                data_useful.at[idx, \"geo_quality\"] = 0\n",
    "\n",
    "# — ПЕРВЫЙ ПРОХОД —\n",
    "mask_blank = data_useful[\"lat\"].isna() & data_useful[\"lon\"].isna()\n",
    "print(\"House-level pass…\")\n",
    "geocode_rows(tqdm(data_useful.loc[mask_blank].iterrows(),\n",
    "                  total=mask_blank.sum(), desc=\"house\"))\n",
    "\n",
    "# — ВТОРОЙ ПРОХОД (центр улицы) —\n",
    "mask_blank = data_useful[\"lat\"].isna() & data_useful[\"lon\"].isna()\n",
    "if mask_blank.any():\n",
    "    streets = (\n",
    "        data_useful.loc[mask_blank, \"street\"]\n",
    "        .str.lower().str.strip().dropna().unique()\n",
    "    )\n",
    "    street_cache: Dict[str, Tuple[float, float]] = {}\n",
    "    print(\"Building street-centroid cache…\")\n",
    "    for st in tqdm(streets, desc=\"street\"):\n",
    "        try:\n",
    "            loc = rate_limited(f\"Россия, Москва, {st}\")\n",
    "        except (GeocoderTimedOut, GeocoderServiceError):\n",
    "            continue\n",
    "        if loc and loc.raw.get(\"type\") in STREET_TYPES:\n",
    "            street_cache[st] = (loc.latitude, loc.longitude)\n",
    "\n",
    "    if street_cache:\n",
    "        print(\"Street-level pass…\")\n",
    "        geocode_rows(tqdm(data_useful.loc[mask_blank].iterrows(),\n",
    "                          total=mask_blank.sum(), desc=\"street\"),\n",
    "                     street_cache=street_cache)\n",
    "\n",
    "# Финальная чистка\n",
    "before = len(data_useful)\n",
    "data_useful.dropna(subset=[\"lat\", \"lon\"], inplace=True)\n",
    "after = len(data_useful)\n",
    "print(f\"Удалено строк без координат: {before - after} (осталось {after})\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "House-level pass…\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "house:   0%|          | 0/16114 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "58e05ceec9f3435fb2962b5e9ac54b84"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building street-centroid cache…\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "street:   0%|          | 0/489 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1319e4e35468484abed334c4b514d7e8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Street-level pass…\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "street:   0%|          | 0/2926 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "330e3b3985494734a35bd7f6f2c518a7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Удалено строк без координат: 980 (осталось 15134)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Заполнение районов и ближайшей станции метро\n",
    "\n",
    "*Что происходит:*  \n",
    "- Создаём GeoDataFrame точек (`lat`, `lon`) и делаем spatial-join с полигонами районов → заполняем `district`.  \n",
    "- Для оставшихся NaN в `underground` используем `metro_tree.query(k=1)` → записываем ближайшую станцию.\n",
    "- Контролируем, чтобы в обоих столбцах не осталось пропусков.\n",
    "\n",
    "___"
   ],
   "metadata": {
    "id": "a6og7ktdPUEa"
   }
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "MkTg6Aw0N0df",
    "outputId": "82688844-b63c-40f3-e6fa-ef4e44d7354c",
    "ExecuteTime": {
     "end_time": "2025-05-24T20:55:29.192618Z",
     "start_time": "2025-05-24T20:55:29.042226Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def enrich_with_districts_and_metro(\n",
    "    data_useful: pd.DataFrame,\n",
    "    districts: gpd.GeoDataFrame,\n",
    "    metro_df: pd.DataFrame,\n",
    "    metro_tree: cKDTree\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Обогащает датафрейм data_useful районом (по координатам) и ближайшим метро (по KD-дереву).\n",
    "    \"\"\"\n",
    "\n",
    "    data_useful = data_useful.loc[:, ~data_useful.columns.duplicated()]\n",
    "\n",
    "    # Собираем GeoDataFrame точек с тем же индексом\n",
    "    points = gpd.GeoDataFrame(\n",
    "        geometry=gpd.points_from_xy(data_useful.lon, data_useful.lat),\n",
    "        crs=\"EPSG:4326\",\n",
    "        index=data_useful.index\n",
    "    )\n",
    "\n",
    "    # Spatial join с полигонами районов\n",
    "    joined = gpd.sjoin(\n",
    "        points,\n",
    "        districts[[\"district_name\", \"geometry\"]],\n",
    "        how=\"left\",\n",
    "        predicate=\"within\"\n",
    "    )\n",
    "\n",
    "    if \"index_left\" in joined.columns:\n",
    "        joined = joined.set_index(\"index_left\", drop=True)\n",
    "\n",
    "    district_cols = [col for col in joined.columns if col == \"district_name\"]\n",
    "\n",
    "    if len(district_cols) > 1:\n",
    "        district_series = joined[district_cols].iloc[:, 0]\n",
    "    else:\n",
    "        district_series = joined[\"district_name\"]\n",
    "\n",
    "    # Группируем по индексу и берём первое значение (на случай дублей геометрий)\n",
    "    district_map = district_series.groupby(level=0).first()\n",
    "\n",
    "    # Заполняем пропуски в столбце district\n",
    "    data_useful[\"district\"] = data_useful[\"district\"].fillna(district_map)\n",
    "\n",
    "    # Теперь подземка через KD-tree\n",
    "    mask_und = data_useful[\"underground\"].isna()\n",
    "    if mask_und.any():\n",
    "        coords = data_useful.loc[mask_und, [\"lat\", \"lon\"]].to_numpy()\n",
    "        _, idx = metro_tree.query(coords, k=1)\n",
    "        data_useful.loc[mask_und, \"underground\"] = metro_df[\"name\"].iloc[idx].to_numpy()\n",
    "\n",
    "    print(\"NaN district     :\", data_useful[\"district\"].isna().sum())\n",
    "    print(\"NaN underground :\", data_useful[\"underground\"].isna().sum())\n",
    "\n",
    "    return data_useful\n",
    "\n",
    "data_useful = enrich_with_districts_and_metro(data_useful, districts, metro_df, metro_tree)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN district     : 32\n",
      "NaN underground : 0\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Очистка и сохранение результата\n",
    "\n",
    "*Что происходит:*  \n",
    "- Удаляем остаточные записи без значения `district`.  \n",
    "- Сохраняем итоговый обогащённый датасет в `cian_rent_Moscow_cleaned.csv` (UTF-8 BOM).\n",
    "\n",
    "---"
   ]
  },
  {
   "metadata": {
    "id": "DeUf30MKN0df",
    "outputId": "108ad0e3-943f-48d1-be03-8496a36ade2c",
    "ExecuteTime": {
     "end_time": "2025-05-24T20:56:26.567603Z",
     "start_time": "2025-05-24T20:56:26.445459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 2: отброс пропусков по district, сохранение\n",
    "data_useful.dropna(subset=[\"district\"], inplace=True)\n",
    "\n",
    "output_path = \"cian_rent_Moscow_cleaned.csv\"\n",
    "data_useful.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"Очищенный датасет сохранён: {len(data_useful):,} строк → {output_path}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Очищенный датасет сохранён: 15,102 строк → cian_rent_Moscow_cleaned.csv\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7. Загрузка данных и функция Haversine\n",
    "\n",
    "*Что происходит:*  \n",
    "- Загружаем очищенный датасет `cian_rent_Moscow_cleaned.csv`.  \n",
    "- Определяем функцию `haversine()` для расчёта расстояния между двумя географическими точками в километрах.  \n",
    "- Эта функция понадобится для расчёта расстояний до центра Москвы и ближайшего метро.\n",
    "\n",
    "---"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T20:56:43.821802Z",
     "start_time": "2025-05-24T20:56:43.764669Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(\"cian_rent_Moscow_cleaned.csv\")\n",
    "\n",
    "def haversine(lat1: float, lon1: float, lat2: float, lon2: float) -> float:\n",
    "    R = 6371.0\n",
    "    φ1, φ2 = math.radians(lat1), math.radians(lat2)\n",
    "    Δφ = math.radians(lat2 - lat1)\n",
    "    Δλ = math.radians(lon2 - lon1)\n",
    "    a = math.sin(Δφ/2)**2 + math.cos(φ1)*math.cos(φ2)*math.sin(Δλ/2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    return R * c"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 8. Расчёт расстояния до центра города\n",
    "\n",
    "*Что происходит:*  \n",
    "- Добавляем колонку `dist_to_center_km`, в которой указано расстояние от квартиры до центра Москвы.  \n",
    "- За центр принимаем координаты Красной площади: `(55.752023, 37.617499)`.  \n",
    "- Используем функцию `haversine()` построчно для расчёта.\n",
    "\n",
    "---"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T20:56:48.374738Z",
     "start_time": "2025-05-24T20:56:48.213077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add_distance_to_center(\n",
    "    df: pd.DataFrame,\n",
    "    center_coords: Tuple[float, float] = (55.752023, 37.617499),\n",
    "    lat_col: str = \"lat\",\n",
    "    lon_col: str = \"lon\",\n",
    "    new_col: str = \"dist_to_center_km\"\n",
    ") -> pd.DataFrame:\n",
    "    center_lat, center_lon = center_coords\n",
    "    df[new_col] = df.apply(\n",
    "        lambda row: haversine(row[lat_col], row[lon_col], center_lat, center_lon),\n",
    "        axis=1\n",
    "    )\n",
    "    return df\n",
    "\n",
    "df = add_distance_to_center(df)"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 9. Нормализация и KD-fallback для станции метро\n",
    "\n",
    "*Что происходит:*  \n",
    "- Загружаем справочник станций метро (`moscow_metro_stations.csv`).  \n",
    "- Нормализуем названия станций:  \n",
    "  - заменяем `ё` на `е`,  \n",
    "  - удаляем слова \"станция\", \"МЦК\", \"МЦД\" и текст в скобках,  \n",
    "  - устраняем лишние пробелы и приводим текст к нижнему регистру.  \n",
    "- Маппим полученные строки на официальные названия станций.  \n",
    "- Для строк, где не удалось найти метро, используем KD-дерево:  \n",
    "  - определяем ближайшую станцию метро по координатам.\n",
    "\n",
    "---"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T20:57:00.349768Z",
     "start_time": "2025-05-24T20:57:00.242121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def normalize_text(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = s.replace(\"ё\", \"е\")\n",
    "    s = re.sub(r\"[—–]+\", \"-\", s)\n",
    "    s = re.sub(r\"\\s*\\(.*?\\)\", \"\", s, flags=re.I)\n",
    "    s = re.sub(r\"\\b(станция|мцк|мцд)\\b\", \"\", s, flags=re.I)\n",
    "    s = re.sub(r\"\\s{2,}\", \" \", s)\n",
    "    return s.strip().lower()\n",
    "\n",
    "def build_station_mapper(metro_df: pd.DataFrame) -> dict:\n",
    "    return {\n",
    "        normalize_text(name): name\n",
    "        for name in metro_df[\"name\"].dropna().unique()\n",
    "    }\n",
    "\n",
    "metro_df = pd.read_csv(\"moscow_metro_stations.csv\")\n",
    "mapper   = build_station_mapper(metro_df)\n",
    "\n",
    "df[\"underground\"] = (\n",
    "    df[\"underground\"]\n",
    "      .fillna(\"\")            \n",
    "      .astype(str)\n",
    "      .apply(normalize_text)\n",
    "      .map(mapper)\n",
    ")\n",
    "\n",
    "mask = df[\"underground\"].isna()\n",
    "if mask.any():\n",
    "    coords = df.loc[mask, [\"lat\", \"lon\"]].to_numpy()\n",
    "    _, idx = metro_tree.query(coords, k=1)\n",
    "    df.loc[mask, \"underground\"] = metro_df[\"name\"].iloc[idx].to_numpy()\n",
    "\n",
    "print(\"Осталось NaN в underground:\", df[\"underground\"].isna().sum())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Осталось NaN в underground: 0\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 10. Расчёт расстояния до метро\n",
    "\n",
    "*Что происходит:*  \n",
    "- Добавляем колонку `dist_to_metro_km` — расстояние от квартиры до указанной станции метро.  \n",
    "- Используем справочник координат станций метро.  \n",
    "- Расчёт выполняется функцией `haversine()`.\n",
    "\n",
    "---"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T20:57:12.352756Z",
     "start_time": "2025-05-24T20:57:12.146919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add_distance_to_metro(\n",
    "    df: pd.DataFrame,\n",
    "    metro_df: pd.DataFrame,\n",
    "    lat_col: str = \"lat\",\n",
    "    lon_col: str = \"lon\",\n",
    "    station_col: str = \"underground\",\n",
    "    new_col: str = \"dist_to_metro_km\"\n",
    ") -> pd.DataFrame:\n",
    "    station_coords: Dict[str, Tuple[float, float]] = {\n",
    "        row[\"name\"]: (row[\"lat\"], row[\"lon\"])\n",
    "        for _, row in metro_df.iterrows()\n",
    "    }\n",
    "    def compute_dist(row):\n",
    "        coords = station_coords.get(row[station_col])\n",
    "        return float(\"nan\") if coords is None else haversine(\n",
    "            row[lat_col], row[lon_col], coords[0], coords[1]\n",
    "        )\n",
    "    df[new_col] = df.apply(compute_dist, axis=1)\n",
    "    return df\n",
    "\n",
    "df = add_distance_to_metro(df, metro_df)\n",
    "print(\"Осталось NaN в dist_to_metro_km:\", df[\"dist_to_metro_km\"].isna().sum())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Осталось NaN в dist_to_metro_km: 0\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 11. Сохранение новой версии датасета\n",
    "\n",
    "*Что происходит:*  \n",
    "- Сохраняем новый датафрейм в файл `cian_rent_Moscow_with_dist.csv`.  \n",
    "- Включены новые полезные признаки: расстояние до центра и растояние до ближайшего метро.  \n",
    "\n",
    "---"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T21:05:21.255088Z",
     "start_time": "2025-05-24T21:05:21.039664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_path = \"cian_rent_Moscow_with_dist.csv\"\n",
    "df.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"Новый датасет сохранён: {len(df):,} строк → {output_path}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Новый датасет сохранён: 15,102 строк → cian_rent_Moscow_with_dist.csv\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 12. Параметры и категории POI\n",
    "\n",
    "*Что происходит:*    \n",
    "- Устанавливаем пути к входным и выходным файлам.  \n",
    "- Задаем категории объектов интереса (POI):  \n",
    "  - зеленые зоны (`GREEN_LEISURE`),  \n",
    "  - исключения для неучебных \"школ\" (`SCHOOL_EXCLUDE`),  \n",
    "  - ключевые слова для аптек и продуктовых магазинов.\n",
    "\n",
    "---"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T20:59:24.614936Z",
     "start_time": "2025-05-24T20:59:24.572625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "PBF_FILE  = \"moscow.pbf\"\n",
    "INPUT_CSV = \"cian_rent_Moscow_with_dist.csv\"\n",
    "OUTPUT_CSV = \"cian_rent_Moscow_with_poi.csv\"\n",
    "\n",
    "GREEN_LEISURE = {\"park\", \"garden\", \"square\", \"recreation_ground\"}\n",
    "SCHOOL_EXCLUDE = {\n",
    "    \"грум\", \"маникюр\", \"танц\", \"автошкол\", \"кулинар\",\n",
    "    \"массаж\", \"церков\", \"приход\", \"духовн\", \"sunday school\",\n",
    "}\n",
    "PHARMA_KEYS = {\n",
    "    \"апрель\", \"ригла\", \"планета здоровья\", \"имплозия\",\n",
    "    \"эркафарм\", \"мелодия здоровья\", \"36,6\", \"неофарм\",\n",
    "    \"ирис\", \"фармленд\", \"вита\",\n",
    "}\n",
    "GROCERY_KEYS = {\n",
    "    \"пятероч\", \"перекресток\", \"чижик\", \"магнит\",\n",
    "    \"красное & белое\", \"бристоль\", \"лента\",\n",
    "    \"светофор\", \"вкусвилл\", \"auchan\",\n",
    "    \"азбука вкуса\", \"globus gourmet\", \"bella vita\", \"dean & deluca\",\n",
    "}\n",
    "transform = pyproj.Transformer.from_crs(4326, 3857, always_xy=True).transform\n",
    "wkt_factory = osmium.geom.WKTFactory()"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 13. Предикаты для фильтрации объектов\n",
    "\n",
    "*Что происходит:*  \n",
    "- Определяем предикаты для фильтрации объектов OSM по тегам:  \n",
    "  - `is_green`: парк, сквер и т.п.  \n",
    "  - `is_school`: образовательные учреждения за вычетом проф. курсов.  \n",
    "  - `is_pharma` и `is_grocery`: проверка на наличие ключевых слов.  \n",
    "  - `is_medical`: поликлиники и больницы.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T20:59:28.457643Z",
     "start_time": "2025-05-24T20:59:28.439513Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def norm(t: str) -> str: return t.lower().replace(\"ё\", \"е\") if t else \"\"\n",
    "\n",
    "def is_green(tags: Dict[str, str]) -> bool:\n",
    "    return tags.get(\"leisure\") in GREEN_LEISURE\n",
    "\n",
    "def is_school(tags: Dict[str, str]) -> bool:\n",
    "    if tags.get(\"amenity\") != \"school\" and tags.get(\"building\") != \"school\":\n",
    "        return False\n",
    "    return not any(w in norm(tags.get(\"name\", \"\")) for w in SCHOOL_EXCLUDE)\n",
    "\n",
    "def has_kw(tags: Dict[str, str], kws: set[str]) -> bool:\n",
    "    joined = \" \".join(tags.get(k, \"\") for k in (\"name\", \"brand\", \"operator\"))\n",
    "    return any(k in norm(joined) for k in kws)\n",
    "\n",
    "def is_pharma(tags: Dict[str, str]) -> bool:\n",
    "    return tags.get(\"amenity\") == \"pharmacy\" and has_kw(tags, PHARMA_KEYS)\n",
    "\n",
    "def is_grocery(tags: Dict[str, str]) -> bool:\n",
    "    if tags.get(\"shop\") not in {\"supermarket\", \"convenience\", \"mall\", \"department_store\"}:\n",
    "        return False\n",
    "    return has_kw(tags, GROCERY_KEYS)\n",
    "\n",
    "def is_medical(tags: Dict[str, str]) -> bool:\n",
    "    return tags.get(\"amenity\") in {\"clinic\", \"hospital\"} or tags.get(\"healthcare\") in {\"clinic\", \"hospital\"}"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 14. OSM Handler и утилиты\n",
    "\n",
    "*Что происходит:*  \n",
    "- Создаем обработчик `POI` для сбора POI из PBF-файла:  \n",
    "  - обрабатываем `node`, `way`, `area` из OSM.  \n",
    "  - сохраняем координаты объектов по категориям.  \n",
    "- Используем `centroid` для вычисления центра многоугольников.  \n",
    "- Добавляем функцию `dedup` для удаления близкорасположенных дубликатов.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T20:59:31.740291Z",
     "start_time": "2025-05-24T20:59:31.714452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def centroid(entity: Any) -> Tuple[float, float]:\n",
    "    try:\n",
    "        if isinstance(entity, osmium.osm.Way) and entity.is_closed():\n",
    "            geom = shapely.wkt.loads(wkt_factory.create_multipolygon(entity))\n",
    "        elif isinstance(entity, osmium.osm.Area):\n",
    "            geom = shapely.wkt.loads(wkt_factory.create_multipolygon(entity))\n",
    "        else:\n",
    "            return entity.location.lon, entity.location.lat\n",
    "        c = geom.centroid\n",
    "        return c.x, c.y\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "class POI(osmium.SimpleHandler):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.parks, self.schools, self.pharm, self.gro, self.med = ([] for _ in range(5))\n",
    "\n",
    "    def _add(self, lon: float, lat: float, tags: Dict[str, str]) -> None:\n",
    "        x, y = transform(lon, lat)\n",
    "        if is_green(tags):\n",
    "            self.parks.append((x, y))\n",
    "        elif is_school(tags):\n",
    "            self.schools.append((x, y))\n",
    "        elif is_pharma(tags):\n",
    "            self.pharm.append((x, y))\n",
    "        elif is_grocery(tags):\n",
    "            self.gro.append((x, y))\n",
    "        elif is_medical(tags):\n",
    "            self.med.append((x, y))\n",
    "\n",
    "    def node(self, n): self._add(n.location.lon, n.location.lat, dict(n.tags))\n",
    "    def way(self, w):\n",
    "        if w.is_closed():\n",
    "            lon, lat = centroid(w)\n",
    "            if lon is not None: self._add(lon, lat, dict(w.tags))\n",
    "    def area(self, a):\n",
    "        lon, lat = centroid(a)\n",
    "        if lon is not None: self._add(lon, lat, dict(a.tags))\n",
    "\n",
    "def dedup(coords: List[Tuple[float, float]], r: float = 100.) -> np.ndarray:\n",
    "    arr = np.asarray(coords)\n",
    "    if not len(arr): return arr.reshape(0, 2)\n",
    "    tree, keep = cKDTree(arr), np.ones(len(arr), bool)\n",
    "    for i, p in enumerate(arr):\n",
    "        if keep[i]:\n",
    "            nbrs = tree.query_ball_point(p, r); nbrs.remove(i); keep[nbrs] = False\n",
    "    return arr[keep]"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 15. Парсинг PBF и агрегация школ\n",
    "\n",
    "*Что происходит:*  \n",
    "- Применяем обработчик POI к файлу `moscow.pbf`.  \n",
    "- Получаем списки координат объектов.  \n",
    "- Удаляем дублирующиеся школы, расположенные ближе 100 метров друг к другу.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T21:04:13.807963Z",
     "start_time": "2025-05-24T20:59:35.137057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "h = POI(); h.apply_file(PBF_FILE, locations=True, idx=\"flex_mem\")\n",
    "schools = dedup(h.schools)"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 16. Обработка квартир и подсчет POI в радиусе\n",
    "\n",
    "*Что происходит:*  \n",
    "- Загружаем датасет с квартирами (`INPUT_CSV`).  \n",
    "- Преобразуем координаты квартир в метры (проекция EPSG:3857).  \n",
    "- Для каждой категории POI считаем количество объектов в радиусе 1 км:  \n",
    "  - `parks_1km`,  \n",
    "  - `groceries_top_1km`,  \n",
    "  - `schools_1km`,  \n",
    "  - `pharmacies_top_1km`,  \n",
    "  - `clinics_1km`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T21:04:39.266742Z",
     "start_time": "2025-05-24T21:04:38.800744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(INPUT_CSV)\n",
    "flat_xy = np.vstack([transform(lon, lat) for lon, lat in zip(df.lon, df.lat)])\n",
    "\n",
    "def add(df: pd.DataFrame, name: str, pts: np.ndarray, r: float) -> None:\n",
    "    if not len(pts): df[name] = 0; return\n",
    "    hits = cKDTree(pts).query_ball_point(flat_xy, r)\n",
    "    df[name] = [len(h) for h in hits]\n",
    "\n",
    "add(df, \"parks_1km\", np.asarray(h.parks), 1000)\n",
    "add(df, \"groceries_top_1km\", np.asarray(h.gro), 1000)\n",
    "add(df, \"schools_1km\", schools, 1000)\n",
    "add(df, \"pharmacies_top_1km\", np.asarray(h.pharm), 1000)\n",
    "add(df, \"clinics_1km\", np.asarray(h.med), 1000)"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T21:05:52.624736Z",
     "start_time": "2025-05-24T21:05:52.472262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_path = \"cian_rent_Moscow_with_poi.csv\"\n",
    "df.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"Финальный датасет сохранён: {len(df):,} строк → {output_path}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Финальный датасет сохранён: 15,102 строк → cian_rent_Moscow_with_poi.csv\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 17. Сохранение артефактов для инференса\n",
    "\n",
    "*Что происходит:*  \n",
    "- Сохраняем все необходимые артефакты, чтобы на этапе инференса не выполнять дорогие операции повторно.  \n",
    "- Это позволяет быстро обогатить новые данные теми же признаками, что использовались при обучении модели.  \n",
    "- Включает следующие объекты:\n",
    "\n",
    "  - Геоданные:\n",
    "    - `districts_processed.geojson` — полигоны районов;\n",
    "    - `station_mapper.json` — нормализованные названия станций метро;\n",
    "    - `street_cache.json` — кэш координат для fallback-геокодинга улиц.\n",
    "  \n",
    "  - Метро:\n",
    "    - `metro_tree.pkl` — KD-дерево для поиска ближайшего метро.\n",
    "  \n",
    "  - POI:\n",
    "    - `.npy` — координаты объектов (парки, школы и т.д.);\n",
    "    - `.pkl` — KD-деревья для быстрого поиска POI в радиусе.\n",
    "\n",
    "*Зачем нужно:*  \n",
    "Эти артефакты позволяют:\n",
    "- моментально определять район по координатам (без spatial join в рантайме),\n",
    "- определять ближайшее метро и расстояние до него,\n",
    "- считать количество POI рядом с объектом,\n",
    "- избавляют от повторного парсинга `.pbf` и геокодирования.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T01:29:46.920291Z",
     "start_time": "2025-05-25T01:29:46.909286Z"
    }
   },
   "cell_type": "code",
   "source": "os.makedirs(\"artifacts\", exist_ok=True)",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T01:29:50.008514Z",
     "start_time": "2025-05-25T01:29:49.273583Z"
    }
   },
   "cell_type": "code",
   "source": [
    "districts = districts.loc[:, ~districts.columns.duplicated()]\n",
    "districts.to_file(\"artifacts/districts_processed.geojson\", encoding=\"utf-8\")"
   ],
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T01:30:40.555952Z",
     "start_time": "2025-05-25T01:30:40.528347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Сохраняем KD-дерево\n",
    "joblib.dump(metro_tree, \"artifacts/metro_tree.pkl\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['artifacts/metro_tree.pkl']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T01:30:55.787497Z",
     "start_time": "2025-05-25T01:30:55.775333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"artifacts/station_mapper.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(mapper, f, ensure_ascii=False, indent=2)"
   ],
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T01:31:07.178055Z",
     "start_time": "2025-05-25T01:31:07.165049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"artifacts/street_cache.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(street_cache, f, ensure_ascii=False, indent=2)"
   ],
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T01:31:18.747836Z",
     "start_time": "2025-05-25T01:31:18.688012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.save(\"artifacts/parks.npy\", np.asarray(h.parks))\n",
    "np.save(\"artifacts/schools.npy\", schools)\n",
    "np.save(\"artifacts/pharmacies.npy\", np.asarray(h.pharm))\n",
    "np.save(\"artifacts/groceries.npy\", np.asarray(h.gro))\n",
    "np.save(\"artifacts/clinics.npy\", np.asarray(h.med))\n",
    "\n",
    "joblib.dump(cKDTree(np.asarray(h.parks)),      \"artifacts/parks_tree.pkl\")\n",
    "joblib.dump(cKDTree(schools),                 \"artifacts/schools_tree.pkl\")\n",
    "joblib.dump(cKDTree(np.asarray(h.pharm)),     \"artifacts/pharmacies_tree.pkl\")\n",
    "joblib.dump(cKDTree(np.asarray(h.gro)),       \"artifacts/groceries_tree.pkl\")\n",
    "joblib.dump(cKDTree(np.asarray(h.med)),       \"artifacts/clinics_tree.pkl\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['artifacts/clinics_tree.pkl']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 54
  }
 ]
}
